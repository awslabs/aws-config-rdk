import argparse
import json
import os
import shutil
import subprocess
import sys
import tempfile

from os import path

# This is the first method to be externalized from rdk.py!

rules_dir = ""  # TODO - why...? Copied this over from rdk.py but don't understand why it is needed


def get_export_parser():
    parser = argparse.ArgumentParser(
        prog="rdk export",
        description="Used to export the Config Rule to terraform file.",
    )
    parser.add_argument(
        "rulename",
        metavar="<rulename>",
        nargs="*",
        help="Rule name(s) to export to a file.",
    )
    parser.add_argument(
        "-s",
        "--rulesets",
        required=False,
        help="comma-delimited list of RuleSet names",
    )
    parser.add_argument(
        "--all",
        "-a",
        action="store_true",
        help="All rules in the working directory will be deployed.",
    )
    parser.add_argument(
        "--lambda-layers",
        required=False,
        help="[optional] Comma-separated list of Lambda Layer ARNs to deploy with your Lambda function(s).",
    )
    parser.add_argument(
        "--lambda-subnets",
        required=False,
        help="[optional] Comma-separated list of Subnets to deploy your Lambda function(s). If specified, you must also specify --lambda-security-groups.",
    )
    parser.add_argument(
        "--lambda-security-groups",
        required=False,
        help="[optional] Comma-separated list of Security Groups to deploy with your Lambda function(s). If specified, you must also specify --lambda-subnets.",
    )
    parser.add_argument(
        "--lambda-timeout",
        required=False,
        default=60,
        help="[optional] Timeout (in seconds) for the lambda function",
        type=str,
    )
    parser.add_argument(
        "--lambda-role-arn",
        required=False,
        help="[optional] Assign existing iam role to lambda functions. If omitted, new lambda role will be created.",
    )
    parser.add_argument(
        "--lambda-role-name",
        required=False,
        help="[optional] Assign existing iam role to lambda functions. If added, will look for a lambda role in the current account with the given name",
    )
    parser.add_argument(
        "--rdklib-layer-arn",
        required=False,
        help="[optional] Lambda Layer ARN that contains the desired rdklib.  Note that Lambda Layers are region-specific.",
    )
    parser.add_argument(
        "-o",
        "--output-version",
        # required=True,
        help="The Terraform version to use when outputting the export",
        choices=[
            "1.x",
        ],
        default="1.x",
    )
    parser.add_argument(
        "-f",
        "--format",
        # required=True,
        help="Export Format",
        choices=["terraform"],
        default="terraform",
    )
    parser.add_argument(
        "-g",
        "--generated-lambda-layer",
        required=False,
        action="store_true",
        help="[optional] Forces rdk deploy to use the Python lambda layer generated by rdk init --generate-lambda-layer",
    )
    parser.add_argument(
        "--custom-layer-name",
        required=False,
        help='[optional] To use with --generated-lambda-layer, forces the flag to look for a specific lambda-layer name. If omitted, "rdklib-layer" will be used',
    )

    return parser


def package_function_code(rdk_instance, rule_name, params):
    class_name = rdk_instance.__class__.__name__
    my_session = getattr(rdk_instance, f"_{class_name}__get_boto_session")()
    if params["SourceRuntime"] == "java8":
        # Do java build and package.
        print("Running Gradle Build for " + rule_name)
        working_dir = os.path.join(os.getcwd(), rules_dir, rule_name)
        command = ["gradle", "build"]
        subprocess.call(command, cwd=working_dir)

        # set source as distribution zip
        s3_src = os.path.join(
            os.getcwd(),
            rules_dir,
            rule_name,
            "build",
            "distributions",
            rule_name + ".zip",
        )

    else:
        print("Zipping " + rule_name)
        # Remove old zip file if it already exists
        package_file_dst = os.path.join(rule_name, rule_name + ".zip")
        getattr(rdk_instance, f"_{class_name}__delete_package_file")(package_file_dst)

        # zip rule code files and upload to s3 bucket
        s3_src_dir = os.path.join(os.getcwd(), rules_dir, rule_name)
        tmp_src = shutil.make_archive(
            os.path.join(tempfile.gettempdir(), rule_name + my_session.region_name),
            "zip",
            s3_src_dir,
        )
        if not (os.path.exists(package_file_dst)):
            shutil.copy(tmp_src, package_file_dst)
        s3_src = os.path.abspath(package_file_dst)
        getattr(rdk_instance, f"_{class_name}__delete_package_file")(tmp_src)

    s3_dst = "/".join((rule_name, rule_name + ".zip"))

    print("Zipping complete.")

    return s3_dst


def parse_export_args(rdk_instance):
    rdk_instance.args = get_export_parser().parse_args(rdk_instance.args.command_args, rdk_instance.args)

    if bool(rdk_instance.args.lambda_security_groups) != bool(rdk_instance.args.lambda_subnets):
        print("You must specify both lambda-security-groups and lambda-subnets, or neither.")
        sys.exit(1)

    # Check rule names to make sure none are too long.  This is needed to catch Rules created before length constraint was added.
    if rdk_instance.args.rulename:
        for name in rdk_instance.args.rulename:
            if len(name) > 128:
                print(
                    f"Error: Found Rule with name over 128 characters: {name} \n Recreate the Rule with a shorter name."
                )
                sys.exit(1)


def export(rdk_instance):
    # Construct mangled names for private methods
    class_name = rdk_instance.__class__.__name__
    rdk_instance.parse_export_args()

    # get the rule names
    # getattr is used to reference private class names from external modules
    rule_names = getattr(rdk_instance, f"_{class_name}__get_rule_list_for_command")("export")

    # run the export code
    print("Running export")

    for rule_name in rule_names:
        rule_params, cfn_tags = getattr(rdk_instance, f"_{class_name}__get_rule_parameters")(rule_name)

        if "SourceIdentifier" in rule_params:
            print("Found Managed Rule, Ignored.")
            print("Export support only Custom Rules.")
            continue

        source_events = []
        if "SourceEvents" in rule_params:
            source_events = [rule_params["SourceEvents"]]

        source_periodic = ""
        if "SourcePeriodic" in rule_params:
            source_periodic = rule_params["SourcePeriodic"]

        combined_input_parameters = {}
        if "InputParameters" in rule_params:
            combined_input_parameters.update(json.loads(rule_params["InputParameters"]))

        if "OptionalParameters" in rule_params:
            # Remove empty parameters
            keys_to_delete = []
            optional_parameters_json = json.loads(rule_params["OptionalParameters"])
            for key, value in optional_parameters_json.items():
                if not value:
                    keys_to_delete.append(key)
            for key in keys_to_delete:
                del optional_parameters_json[key]
            combined_input_parameters.update(optional_parameters_json)

        print("Found Custom Rule.")
        s3_src = ""
        s3_dst = rdk_instance.package_function_code(rule_name, rule_params)

        layers = []
        rdk_lib_version = "0"
        my_session = getattr(rdk_instance, f"_{class_name}__get_boto_session")()
        layers = getattr(rdk_instance, f"_{class_name}__get_lambda_layers")(my_session, rdk_instance.args, rule_params)

        if rdk_instance.args.lambda_layers:
            additional_layers = rdk_instance.args.lambda_layers.split(",")
            layers.extend(additional_layers)

        subnet_ids = []
        security_group_ids = []
        if rdk_instance.args.lambda_security_groups:
            security_group_ids = rdk_instance.args.lambda_security_groups.split(",")

        if rdk_instance.args.lambda_subnets:
            subnet_ids = rdk_instance.args.lambda_subnets.split(",")

        lambda_role_arn = ""
        if rdk_instance.args.lambda_role_arn:
            print("Existing IAM Role provided: " + rdk_instance.args.lambda_role_arn)
            lambda_role_arn = rdk_instance.args.lambda_role_arn

        my_params = {
            "rule_name": rule_name,
            "rule_lambda_name": getattr(rdk_instance, f"_{class_name}__get_lambda_name")(rule_name, rule_params),
            "source_runtime": getattr(rdk_instance, f"_{class_name}__get_runtime_string")(rule_params),
            "source_events": source_events,
            "source_periodic": source_periodic,
            "source_input_parameters": json.dumps(combined_input_parameters),
            "source_handler": getattr(rdk_instance, f"_{class_name}__get_handler")(rule_name, rule_params),
            "subnet_ids": subnet_ids,
            "security_group_ids": security_group_ids,
            "lambda_layers": layers,
            "lambda_role_arn": lambda_role_arn,
            "lambda_timeout": str(rdk_instance.args.lambda_timeout),
        }

        params_file_path = os.path.join(
            os.getcwd(),
            rules_dir,
            rule_name,
            rule_name.lower() + ".tfvars.json",
        )
        parameters_file = open(
            params_file_path,
            "w",
        )
        json.dump(
            my_params,
            parameters_file,
            indent=4,
        )
        parameters_file.close()
        # create json of CFN template
        print(rdk_instance.args.format + " version: " + rdk_instance.args.output_version)
        tf_file_body = os.path.join(
            path.dirname(__file__),
            "template",
            rdk_instance.args.format,
            rdk_instance.args.output_version,
            "config_rule.tf",
        )
        tf_file_path = os.path.join(os.getcwd(), rules_dir, rule_name, rule_name.lower() + "_rule.tf")
        shutil.copy(tf_file_body, tf_file_path)

        variables_file_body = os.path.join(
            path.dirname(__file__),
            "template",
            rdk_instance.args.format,
            rdk_instance.args.output_version,
            "variables.tf",
        )
        variables_file_path = os.path.join(os.getcwd(), rules_dir, rule_name, rule_name.lower() + "_variables.tf")
        shutil.copy(variables_file_body, variables_file_path)
        print("Export completed. This generated the .tf files required to deploy this rule.")
